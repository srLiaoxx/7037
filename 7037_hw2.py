# -*- coding: utf-8 -*-
"""7037_hw2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vL-BeYgEtlcktqHC55DsDlalRUpZyi7l
"""



from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
import numpy as np

import gc

def question_2_correlation():
    """
    Question 2: Calculate correlations between intraday, overnight, and regular momentum
    """
    print("\nQuestion 2: Momentum Correlations")
    print("=" * 50)

    df = pd.read_parquet('/content/drive/MyDrive/data_for_7037/hw2_mfin7037_data.parquet')
    momentum_cols = ['mom_intraday', 'mom_overnight', 'mom']
    corr_matrix = df[momentum_cols].corr()

    print("\nBenchmark Data Range:")
    print(f"Start Date: {df['date'].min()}")
    print(f"End Date: {df['date'].max()}")

    print("\nCorrelation Matrix:")
    print(corr_matrix)
    return df

import pandas as pd
momentum_columns = ['mom_intraday', 'mom', 'mom_overnight']
correlation_matrix = df[momentum_columns].corr()
print(correlation_matrix)

import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np
correlation_matrix = np.array([[1.000000, 0.591998, -0.691953],
                               [0.591998, 1.000000, 0.105443],
                               [-0.691953, 0.105443, 1.000000]])
sns.heatmap(correlation_matrix, annot=True, cmap="coolwarm", xticklabels=["mom_intraday", "mom", "mom_overnight"], yticklabels=["mom_intraday", "mom", "mom_overnight"])
plt.title('Correlation Heatmap of Momentum Factors')
plt.show()

def calculate_tesla_momentum():
    """
    Question 3: Calculate Tesla momentum using CRSP DSF
    """
    print("\nQuestion 3: Tesla Momentum Calculation")
    print("=" * 50)

    # Get benchmark timeframe
    benchmark_start, benchmark_end = df['date'].min(), df['date'].max()
    print(f"\nBenchmark Period: {benchmark_start} to {benchmark_end}")

    # Load and process Tesla data
    tesla_data = pd.read_parquet(
        '/content/drive/MyDrive/data_for_7037/crsp_202401.dsf.parquet',
        filters=[('permno', '=', 93436)],
        columns=['date', 'permno', 'prc', 'openprc']
    )

    tesla_data['date'] = pd.to_datetime(tesla_data['date'])
    tesla_data = tesla_data.sort_values('date')

    # Filter to match benchmark timeframe
    tesla_data = tesla_data[
        (tesla_data['date'] >= benchmark_start) &
        (tesla_data['date'] <= benchmark_end)
    ]

    # Calculate returns
    tesla_data['intraday_ret'] = tesla_data['prc'].abs() / tesla_data['openprc'] - 1
    tesla_data['overnight_ret'] = tesla_data['openprc'] / tesla_data['prc'].shift(1).abs() - 1

    # Convert to monthly data
    monthly_data = tesla_data.set_index('date').resample('ME').agg({
        'prc': 'last',
        'intraday_ret': lambda x: np.sum(np.log1p(x)),
        'overnight_ret': lambda x: np.sum(np.log1p(x))
    }).reset_index()

    # Calculate momentum
    monthly_data['mom_intraday'] = monthly_data['intraday_ret'].rolling(window=5, min_periods=5).sum().shift(2)
    monthly_data['mom_overnight'] = monthly_data['overnight_ret'].rolling(window=5, min_periods=5).sum().shift(2)

    print("\nLast 5 Months of Calculated Data:")
    print(monthly_data.tail()[['date', 'prc', 'mom_intraday', 'mom_overnight']])

    return monthly_data

def verify_calculations():
    """
    Verify calculations against benchmark data
    """
    # Get benchmark data
    benchmark_df = question_2_correlation()
    benchmark_tesla = benchmark_df[benchmark_df['permno'] == 93436].copy()

    # Get calculated data
    calculated_df = calculate_tesla_momentum()

    # Get the last month for comparison
    benchmark_last = benchmark_tesla.iloc[-1]
    calculated_last = calculated_df.iloc[-1]

    print("\nVerification of End-of-Sample Values:")
    print(f"Date: {calculated_last['date'].strftime('%Y-%m-%d')}")

    print("\nIntraday Momentum:")
    print(f"Benchmark: {benchmark_last['mom_intraday']:.6f}")
    print(f"Calculated: {calculated_last['mom_intraday']:.6f}")
    print(f"Difference: {(calculated_last['mom_intraday'] - benchmark_last['mom_intraday']):.6f}")

    print("\nOvernight Momentum:")
    print(f"Benchmark: {benchmark_last['mom_overnight']:.6f}")
    print(f"Calculated: {calculated_last['mom_overnight']:.6f}")
    print(f"Difference: {(calculated_last['mom_overnight'] - benchmark_last['mom_overnight']):.6f}")

def main():
    drive.mount('/content/drive')
    verify_calculations()

if __name__ == "__main__":
    main()



### Q4:Now report the 12 row x 11 column table, where the
### columns are the average returns/t-staSsScs of those returns for porholios 1..10 and
### then the long-short porholio, and there are six sets of rows, one for EW, VW, EW
### overnight, VW overnight, EW intraday, EW overnight.

# install fastparquet via system call
!pip3 install fastparquet
!pip3 install statsmodels
!pip3 install matplotlib
import fastparquet
import pandas as pd, os
import numpy as np

# set worknig directory to be wherever your data is
try:
    os.chdir('/Volumes/evo/Dropbox (Personal)/teaching_2025_scratch')
except:
    print("need to point to the rigth directory")

df = df.query("date>='1994-01-01'")
df.head()

df['prc'] = df['prc_lag1'].shift(-1)
data = df
df = df[(df['mcap_bin'] > 2) & (df['prc'] >= 5)]

def produce_table(input_data, subsetting=lambda df: df):
    """
    Combines portfolio data from 'bins' and 'pnl', computes summary statistics
    (mean returns multiplied by 100 and one-sample t-test statistics) for each portfolio,
    and returns a pivoted table for portfolios 1..10 and a long-short portfolio.

    Parameters:
        input_data (dict): Dictionary with keys:
            - 'bins': a DataFrame with columns ['date','bin','ew','vw',
                      'ew_intraday','vw_intraday','ew_overnight','vw_overnight']
            - 'pnl': a DataFrame with columns ['date','ew','vw',
                      'ew_intraday','vw_intraday','ew_overnight','vw_overnight']
                      (this will be used to compute the long-short portfolio)
        subsetting (function): A function to subset/modify the combined DataFrame.
                               Defaults to the identity function.

    Returns:
        pd.DataFrame: A table where each row corresponds to a return measure (e.g., EW, VW, etc.)
                      with the first row showing the mean (×100, rounded to 3 decimals) and the second
                      row showing the t-statistic (in parentheses). The long-short portfolio is labeled "10-1".
    """
    # Extract the required columns from each DataFrame.
    bins_df = input_data['bins'][['date', 'bin', 'ew', 'vw',
                                  'ew_intraday', 'vw_intraday',
                                  'ew_overnight', 'vw_overnight']].copy()
    pnl_df = input_data['pnl'][['date', 'ew', 'vw',
                                'ew_intraday', 'vw_intraday',
                                'ew_overnight', 'vw_overnight']].copy()
    # Designate the pnl data as portfolio "11" (which later will be renamed to "10-1")
    pnl_df['bin'] = 11

    # Combine the two datasets and apply any subsetting
    combined = pd.concat([bins_df, pnl_df], ignore_index=True)
    combined = subsetting(combined)

    # List of return measure columns
    cols = ['ew', 'vw', 'ew_intraday', 'vw_intraday', 'ew_overnight', 'vw_overnight']

    # Function to compute mean multiplied by 100 and rounded to 3 decimals
    def meanna(x):
        return round(x.mean() * 100, 3)

    # Compute group-wise means by portfolio (bin)
    s1 = combined.groupby('bin')[cols].agg(meanna).reset_index()
    s1_melt = s1.melt(id_vars='bin', var_name='variable', value_name='value')
    s1_pivot = s1_melt.pivot(index='variable', columns='bin', values='value').reset_index()
    s1_pivot['order'] = 1

    # Function to compute one-sample t-test statistic (null: mean=0), rounded to 3 decimals and wrapped in parentheses.
    def ttesting(x):
        x = x.dropna()
        if len(x) == 0:
            return None
        stat, _ = ttest_1samp(x, popmean=0)
        return f"({round(stat, 3)})"

    # Compute group-wise t-statistics by portfolio (bin)
    s2 = combined.groupby('bin')[cols].agg(ttesting).reset_index()
    s2_melt = s2.melt(id_vars='bin', var_name='variable', value_name='value')
    s2_pivot = s2_melt.pivot(index='variable', columns='bin', values='value').reset_index()
    s2_pivot['order'] = 2

    # Combine the mean and t-statistic tables
    table = pd.concat([s1_pivot, s2_pivot], ignore_index=True)
    table = table.sort_values(by=['variable', 'order'])
    # For rows with t-statistics (order 2), clear the 'variable' name
    table.loc[table['order'] == 2, 'variable'] = ''
    table = table.drop(columns=['order'])

    # Format the variable names: uppercase and replace underscores with spaces
    table['variable'] = table['variable'].str.upper().str.replace('_', ' ', regex=False)
    table = table.rename(columns={'variable': 'Portfolio'})

    # Rename portfolio 11 to "10-1" (i.e. the long-short portfolio)
    if 11 in table.columns:
        table = table.rename(columns={11: '10-1'})

    # Ensure that only portfolios 1 through 10 and the long-short ("10-1") remain.
    valid_portfolios = list(range(1, 11)) + ['10-1']
    cols_to_keep = ['Portfolio'] + [col for col in table.columns if col in valid_portfolios]
    table = table[cols_to_keep]

    return table


# --- Helper: Apply quantiles within each date group ---
def apply_quantiles(df, col, bins=10):
    """
    Assigns a quantile-based bin (1,...,bins) for each value in `col`
    within each date group.
    """
    def quantile_bin(s):
        # Use pd.qcut to get quantile bins; if duplicate edges occur, use rank instead.
        try:
            return pd.qcut(s, q=bins, labels=False, duplicates="drop") + 1
        except Exception:
            return np.ceil(s.rank(method='average') / len(s) * bins)
    return df.groupby('date')[col].transform(quantile_bin)

import matplotlib.pyplot as plt

from scipy.stats import ttest_1samp
import statsmodels.formula.api as smf


def routine_pnl(dta, plot=False):
    """
    Computes portfolio returns (EW/VW) by bin, optionally creates two plots
    (mean returns by bin and cumulative pnl), runs a series of OLS regressions,
    and returns a dictionary with outputs.

    Assumes dta contains columns: date, bin, ret, mcap_lag1,
    intraday_ret_month, overnight_ret_month.

    Parameters:
      dta   : pd.DataFrame
              Input data.
      plot  : bool, default False
              If True, plots are created; if False, they are suppressed.

    Returns:
      dict with keys:
          'p1'             : Axes for decile plot (or None if plot==False)
          'pnl_curve'      : Axes for cumulative pnl plot (or None if plot==False)
          'factor_loadings': Dictionary of regression results
          'pnl'            : DataFrame of pnl calculations
          'bins'           : DataFrame of computed bin returns
    """
    # Ensure date is datetime
    dta = dta.copy()
    dta['date'] = pd.to_datetime(dta['date'])

    # Helper: Weighted mean
    def weighted_mean(x, w):
        return np.average(x, weights=w)

    # Compute portfolio returns by date and bin
    bins = (dta.groupby(['date', 'bin'])
            .apply(lambda g: pd.Series({
                'ew': g['ret'].mean(),
                'vw': weighted_mean(g['ret'], g['mcap_lag1']),
                'ew_intraday': g['intraday_ret_month'].mean() if 'intraday_ret_month' in g.columns else np.nan,
                'vw_intraday': weighted_mean(g['intraday_ret_month'], g['mcap_lag1']) if 'intraday_ret_month' in g.columns else np.nan,
                'ew_overnight': g['overnight_ret_month'].mean() if 'overnight_ret_month' in g.columns else np.nan,
                'vw_overnight': weighted_mean(g['overnight_ret_month'], g['mcap_lag1']) if 'overnight_ret_month' in g.columns else np.nan,
            }))
            .reset_index())

    # Create decile plot (collapsed across dates) if plotting is requested
    if plot:
        bins_collapsed = bins.groupby('bin')[['ew','vw']].mean().reset_index()
        fig1, ax1 = plt.subplots()
        ax1.plot(bins_collapsed['bin'], bins_collapsed['ew'], label='EW', marker='')
        ax1.plot(bins_collapsed['bin'], bins_collapsed['vw'], label='VW', marker='')
        ax1.set_xlabel('Bin')
        ax1.set_ylabel('Return')
        ax1.set_title('Mean Portfolio Returns by Bin')
        ax1.legend()
        plt.tight_layout()
    else:
        ax1 = None

    # Compute long-short pnl using the extreme bins (assume bin 1 and max(bin))
    max_bin = bins['bin'].max()
    selected = bins[bins['bin'].isin([1, max_bin])].copy()
    selected['weight'] = selected['bin'].apply(lambda x: -1 if x == 1 else 1)
    pnl = (selected.groupby('date')
           .apply(lambda g: pd.Series({
               'ew': (g['weight'] * g['ew']).sum(),
               'vw': (g['weight'] * g['vw']).sum(),
               'ew_intraday': (g['weight'] * g['ew_intraday']).sum() if 'ew_intraday' in g.columns else np.nan,
               'vw_intraday': (g['weight'] * g['vw_intraday']).sum() if 'vw_intraday' in g.columns else np.nan,
               'ew_overnight': (g['weight'] * g['ew_overnight']).sum() if 'ew_overnight' in g.columns else np.nan,
               'vw_overnight': (g['weight'] * g['vw_overnight']).sum() if 'vw_overnight' in g.columns else np.nan,
           })).reset_index())
    pnl = pnl.sort_values('date')
    pnl['cumvw'] = (1 + pnl['vw']).cumprod() - 1
    pnl['cumew'] = (1 + pnl['ew']).cumprod() - 1

    # Create cumulative pnl plot if plotting is requested
    if plot:
        fig2, ax2 = plt.subplots()
        ax2.plot(pnl['date'], pnl['cumew'], label='EW')
        ax2.plot(pnl['date'], pnl['cumvw'], label='VW')
        ax2.set_xlabel('Date')
        ax2.set_ylabel('Cumulative Return')
        ax2.set_title('Cumulative PnL Over Time')
        ax2.legend()
        plt.tight_layout()
    else:
        ax2 = None

    # Minimal regressions for demonstration, not done tho
    formulas = [
        'ew ~ 1',
        'vw ~ 1',
        'ew ~ 0',  # dummy formula if needed
    ]
    regs = {}
    for bin_val in sorted(bins['bin'].unique()):
        data_bin = bins[bins['bin'] == bin_val]
        for fml in formulas:
            key = f'bin {bin_val} fml {fml}'
            try:
                model = smf.ols(formula=fml, data=data_bin).fit()
            except Exception as e:
                model = None
                print(f"Regression failed for bin {bin_val} with formula '{fml}': {e}")
            regs[key] = model

    # Return outputs in a dictionary
    return {
        'p1': ax1,                # decile plot (or None)
        'pnl_curve': ax2,         # cumulative pnl plot (or None)
        'factor_loadings': regs,  # regression results
        'pnl': pnl,               # pnl table
        'bins': bins              # bins table (with computed returns)
    }

# --- Strategy 1 ---
# (Following the R code, first a ranking step is done then overwritten by quantiles on mom_intraday)
res2_strategy1 = df.copy()
# Here we assign bin using mom_intraday quantiles (10 bins)
res2_strategy1['bin'] = apply_quantiles(res2_strategy1, 'mom_overnight', bins=10)
subset1 = res2_strategy1.dropna(subset=['bin', 'mcap_lag1', 'ret'])
strat1 = routine_pnl(subset1)
table1 = produce_table({'bins': strat1['bins'], 'pnl': strat1['pnl']})
print("=== Strategy 1 Table ===")
# print the table
#  order by index and plot table
table1

# --- Strategy 2 ---
# (Following the R code, first a ranking step is done then overwritten by quantiles on mom_intraday)
res2_strategy1 = df.copy()
# Here we assign bin using mom_intraday quantiles (10 bins)
res2_strategy1['bin'] = apply_quantiles(res2_strategy1, 'mom_intraday', bins=10)
subset1 = res2_strategy1.dropna(subset=['bin', 'mcap_lag1', 'ret'])
strat1 = routine_pnl(subset1)
table1 = produce_table({'bins': strat1['bins'], 'pnl': strat1['pnl']})
print("=== Strategy 1 Table ===")
# print the table
#  order by index and plot table
table1

# --- Strategy 3 ---
# (Following the R code, first a ranking step is done then overwritten by quantiles on mom_intraday)
res2_strategy1 = df.copy()
# Here we assign bin using mom_intraday quantiles (10 bins)
res2_strategy1['bin'] = apply_quantiles(res2_strategy1, 'mom', bins=10)
subset1 = res2_strategy1.dropna(subset=['bin', 'mcap_lag1', 'ret'])
strat1 = routine_pnl(subset1)
table1 = produce_table({'bins': strat1['bins'], 'pnl': strat1['pnl']})
print("=== Strategy 3 Table ===")
# print the table
#  order by index and plot table
table1



### Q4 extra:

import pyarrow.parquet as pq
parquet_file = pq.ParquetFile('/content/drive/MyDrive/data_for_7037/ff.five_factor.parquet')
ff_factors = parquet_file.read().to_pandas()
ff_factors['date'] = pd.to_datetime(ff_factors['dt'])

momentum_data = data
print(momentum_data.head())

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

momentum_data['date'] = pd.to_datetime(momentum_data['date'], errors='coerce')
momentum_data = pd.merge(momentum_data, ff_factors[['date', 'mkt_rf']], on='date', how='inner')

winners = momentum_data[momentum_data['mcap_bin'] == 10]

monthly_winners = winners.groupby(winners['date'].dt.to_period('M'))['ret'].mean()

monthly_market_return = momentum_data.groupby(momentum_data['date'].dt.to_period('M'))['mkt_rf'].mean()

overperforming_winners_ratio = (monthly_winners > monthly_market_return).mean()

print(f"Fraction of winners outperforming the market: {overperforming_winners_ratio}")

plt.figure(figsize=(10, 6))
plt.plot(monthly_winners.index.astype(str), monthly_winners, label='Winners Portfolio Return', color='blue')
plt.plot(monthly_market_return.index.astype(str), monthly_market_return, label='Market Return (MKT)', color='red')
plt.title('Monthly Return of Winners vs Market Return')
plt.xlabel('Month')
plt.ylabel('Return')
plt.legend(loc='best')

plt.xticks(ticks=monthly_winners.index[::3].astype(str), rotation=45)

plt.tight_layout()
plt.show()

plt.figure(figsize=(10, 6))
overperforming_winners_ratio_monthly = (monthly_winners > monthly_market_return).rolling(window=12).mean()  # 12个月滚动平均
plt.plot(overperforming_winners_ratio_monthly.index.astype(str), overperforming_winners_ratio_monthly, label='Rolling 12-Month Fraction of Winners Outperforming Market')
plt.title('Rolling 12-Month Fraction of Winners Outperforming the Market')
plt.xlabel('Month')
plt.ylabel('Fraction')
plt.legend(loc='best')

plt.xticks(ticks=overperforming_winners_ratio_monthly.index[::3].astype(str), rotation=45)

plt.tight_layout()
plt.show()

momentum_data = data

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

def compute_winners_fraction(momentum_data, ff_factors):
    momentum_data['date'] = pd.to_datetime(momentum_data['date'])
    ff_factors['date'] = pd.to_datetime(ff_factors['dt'])

    momentum_data['quantile'] = momentum_data.groupby('date')['mom'].transform(lambda x: pd.qcut(x, 5, labels=False) + 1)

    winners_data = momentum_data[momentum_data['quantile'] == 5]

    winners_data = pd.merge(winners_data, ff_factors[['date', 'mkt_rf']], on='date', how='left')

    winners_data['outperform'] = winners_data['ret'] > winners_data['mkt_rf']

    monthly_performance = winners_data.groupby('date')['outperform'].mean()

    avg_winners_fraction = monthly_performance.mean()

    return avg_winners_fraction, monthly_performance

avg_winners_fraction, monthly_performance = compute_winners_fraction(momentum_data, ff_factors)
print(f"Average fraction of winners outperforming the market: {avg_winners_fraction:.4f}")

plt.figure(figsize=(10, 6))
plt.plot(monthly_performance.index.astype(str), monthly_performance, label='Fraction of Winners Outperforming the Market', color='green')
plt.title('Monthly Fraction of Winners Outperforming the Market')
plt.xlabel('Month')
plt.ylabel('Fraction')
plt.legend(loc='best')

plt.xticks(ticks=monthly_performance.index[::3].astype(str), rotation=45)

plt.tight_layout()
plt.show()

momentum_data = data

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

def compute_winners_fraction(momentum_data, ff_factors):
    momentum_data['date'] = pd.to_datetime(momentum_data['date'])
    ff_factors['date'] = pd.to_datetime(ff_factors['dt'])

    momentum_data['quantile'] = momentum_data.groupby('date')['mom_overnight'].transform(lambda x: pd.qcut(x, 5, labels=False) + 1)

    winners_data = momentum_data[momentum_data['quantile'] == 5]

    winners_data = pd.merge(winners_data, ff_factors[['date', 'mkt_rf']], on='date', how='left')

    winners_data['outperform'] = winners_data['ret'] > winners_data['mkt_rf']

    monthly_performance = winners_data.groupby('date')['outperform'].mean()

    avg_winners_fraction = monthly_performance.mean()

    return avg_winners_fraction, monthly_performance

avg_winners_fraction, monthly_performance = compute_winners_fraction(momentum_data, ff_factors)
print(f"Average fraction of winners outperforming the market: {avg_winners_fraction:.4f}")

plt.figure(figsize=(10, 6))
plt.plot(monthly_performance.index.astype(str), monthly_performance, label='Fraction of Winners Outperforming the Market', color='green')
plt.title('Monthly Fraction of Winners Outperforming the Market')
plt.xlabel('Month')
plt.ylabel('Fraction')
plt.legend(loc='best')

plt.xticks(ticks=monthly_performance.index[::3].astype(str), rotation=45)

plt.tight_layout()
plt.show()

momentum_data = data

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

def compute_winners_fraction(momentum_data, ff_factors):
    momentum_data['date'] = pd.to_datetime(momentum_data['date'])
    ff_factors['date'] = pd.to_datetime(ff_factors['dt'])

    momentum_data['quantile'] = momentum_data.groupby('date')['mom_intraday'].transform(lambda x: pd.qcut(x, 5, labels=False) + 1)

    winners_data = momentum_data[momentum_data['quantile'] == 5]

    winners_data = pd.merge(winners_data, ff_factors[['date', 'mkt_rf']], on='date', how='left')

    winners_data['outperform'] = winners_data['ret'] > winners_data['mkt_rf']

    monthly_performance = winners_data.groupby('date')['outperform'].mean()

    avg_winners_fraction = monthly_performance.mean()

    return avg_winners_fraction, monthly_performance

avg_winners_fraction, monthly_performance = compute_winners_fraction(momentum_data, ff_factors)
print(f"Average fraction of winners outperforming the market: {avg_winners_fraction:.4f}")

plt.figure(figsize=(10, 6))
plt.plot(monthly_performance.index.astype(str), monthly_performance, label='Fraction of Winners Outperforming the Market', color='green')
plt.title('Monthly Fraction of Winners Outperforming the Market')
plt.xlabel('Month')
plt.ylabel('Fraction')
plt.legend(loc='best')

plt.xticks(ticks=monthly_performance.index[::3].astype(str), rotation=45)

plt.tight_layout()
plt.show()



### total

momentum_data = data

import pandas as pd

def compute_winners_fraction(momentum_data, ff_factors):
    momentum_data['date'] = pd.to_datetime(momentum_data['date'])
    ff_factors['date'] = pd.to_datetime(ff_factors['dt'])

    momentum_data['quantile'] = momentum_data.groupby('date')['mom'].transform(lambda x: pd.qcut(x, 5, labels=False) + 1)
    winners_data = momentum_data[momentum_data['quantile'] == 5]
    winners_data = pd.merge(winners_data, ff_factors[['date', 'mkt_rf']], on='date', how='left')

    winners_data['outperform'] = winners_data['ret'] > winners_data['mkt_rf']

    monthly_performance = winners_data.groupby('date')['outperform'].mean()

    avg_winners_fraction = monthly_performance.mean()

    return avg_winners_fraction

avg_winners_fraction = compute_winners_fraction(momentum_data, ff_factors)
print(f"Average fraction of winners outperforming the market: {avg_winners_fraction:.4f}")





#### Q5: Now compute an “independent sort”, whereby we sort independently on intraday momentum, overnight momentum.

momentum_data = data
momentum_data = momentum_data.dropna()

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

def independent_sort(momentum_data):
    momentum_data['date'] = pd.to_datetime(momentum_data['date'])

    momentum_data['intraday_bin'] = momentum_data.groupby('date')['mom_intraday'].transform(
        lambda x: pd.qcut(x, 5, labels=False, duplicates='drop') + 1)
    momentum_data['overnight_bin'] = momentum_data.groupby('date')['mom_overnight'].transform(
        lambda x: pd.qcut(x, 5, labels=False, duplicates='drop') + 1)

    ew_grid = np.zeros((5, 5))
    vw_grid = np.zeros((5, 5))

    for date, group in momentum_data.groupby('date'):
        for i in range(1, 6):
            for j in range(1, 6):
                current_bin = group[(group['intraday_bin'] == i) & (group['overnight_bin'] == j)]

                ew_return = current_bin['ret'].mean()
                vw_return = np.average(current_bin['ret'], weights=current_bin['mcap_lag1'])

                ew_grid[i-1, j-1] += ew_return
                vw_grid[i-1, j-1] += vw_return

    num_months = len(momentum_data['date'].unique())
    ew_grid /= num_months
    vw_grid /= num_months

    return ew_grid, vw_grid

ew_grid, vw_grid = independent_sort(momentum_data)
print("Equal-Weighted Grid:")
print(ew_grid)

print("Value-Weighted Grid:")
print(vw_grid)

fig, ax = plt.subplots(1, 2, figsize=(12, 6))

ax[0].imshow(ew_grid, cmap='coolwarm', interpolation='nearest')
ax[0].set_title('Equal-Weighted Return Grid')
ax[0].set_xticks(np.arange(5))
ax[0].set_yticks(np.arange(5))
ax[0].set_xticklabels(['Q1', 'Q2', 'Q3', 'Q4', 'Q5'])
ax[0].set_yticklabels(['Q1', 'Q2', 'Q3', 'Q4', 'Q5'])
ax[0].set_xlabel('Overnight Momentum Quantile')
ax[0].set_ylabel('Intraday Momentum Quantile')

ax[1].imshow(vw_grid, cmap='coolwarm', interpolation='nearest')
ax[1].set_title('Value-Weighted Return Grid')
ax[1].set_xticks(np.arange(5))
ax[1].set_yticks(np.arange(5))
ax[1].set_xticklabels(['Q1', 'Q2', 'Q3', 'Q4', 'Q5'])
ax[1].set_yticklabels(['Q1', 'Q2', 'Q3', 'Q4', 'Q5'])
ax[1].set_xlabel('Overnight Momentum Quantile')
ax[1].set_ylabel('Intraday Momentum Quantile')

plt.tight_layout()
plt.show()

### Q6:maybe some influence



def analyze_momentum_explanations(momentum_data):
    overnight_groups = momentum_data.groupby('overnight_bin')
    intraday_explains_overnight = {}
    for overnight_bin, group in overnight_groups:
        avg_return_by_intraday = group.groupby('intraday_bin')['ret'].mean()
        intraday_explains_overnight[overnight_bin] = avg_return_by_intraday

    intraday_groups = momentum_data.groupby('intraday_bin')
    overnight_explains_intraday = {}
    for intraday_bin, group in intraday_groups:
        avg_return_by_overnight = group.groupby('overnight_bin')['ret'].mean()
        overnight_explains_intraday[intraday_bin] = avg_return_by_overnight

    combined_grid = np.zeros((5, 5))
    for i in range(1, 6):
        for j in range(1, 6):
            group = momentum_data[(momentum_data['intraday_bin'] == i) & (momentum_data['overnight_bin'] == j)]
            combined_grid[i-1, j-1] = group['ret'].mean()

    return {
        'intraday_explains_overnight': intraday_explains_overnight,
        'overnight_explains_intraday': overnight_explains_intraday,
        'combined_grid': combined_grid
    }

analysis_results = analyze_momentum_explanations(momentum_data)
print(analysis_results)

import statsmodels.api as sm

for overnight_bin in range(1, 6):
    subset = momentum_data[momentum_data['overnight_bin'] == overnight_bin]
    X = subset['mom_intraday']  # intraday momentum
    X = sm.add_constant(X)
    y = subset['ret']

    model = sm.OLS(y, X).fit()
    print(f"Overnight bin {overnight_bin} - Intraday momentum regression:")
    print(model.summary())

for intraday_bin in range(1, 6):
    subset = momentum_data[momentum_data['intraday_bin'] == intraday_bin]
    X = subset['mom_overnight']  # overnight momentum
    X = sm.add_constant(X)
    y = subset['ret']

    model = sm.OLS(y, X).fit()
    print(f"Intraday bin {intraday_bin} - Overnight momentum regression:")
    print(model.summary())





### Q7: Does the strategy which shorts stocks which are low quintile of both momentum
### strategies, and long the high quintile of both momentum strategies (i.e. long the boWom
### right corner, short the top lem corner), perform well?

long_subset = momentum_data[(momentum_data['intraday_bin'] == 5) & (momentum_data['overnight_bin'] == 5)]
short_subset = momentum_data[(momentum_data['intraday_bin'] == 1) & (momentum_data['overnight_bin'] == 1)]

long_return = long_subset['ret'].mean()
short_return = short_subset['ret'].mean()
strategy_return = long_return - short_return
print(f"Long-Short Strategy Return Sum: {strategy_return}")

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

high_momentum_stocks = momentum_data[(momentum_data['intraday_bin'] == 5) & (momentum_data['overnight_bin'] == 5)]
low_momentum_stocks = momentum_data[(momentum_data['intraday_bin'] == 1) & (momentum_data['overnight_bin'] == 1)]

high_momentum_stocks['date'] = pd.to_datetime(high_momentum_stocks['date'])
low_momentum_stocks['date'] = pd.to_datetime(low_momentum_stocks['date'])

high_momentum_stocks_monthly = high_momentum_stocks.groupby(['date', 'permno'])['ret'].mean().reset_index()
low_momentum_stocks_monthly = low_momentum_stocks.groupby(['date', 'permno'])['ret'].mean().reset_index()

high_momentum_stocks_monthly = pd.merge(high_momentum_stocks_monthly, high_momentum_stocks[['date', 'permno', 'mcap_lag1']], on=['date', 'permno'])
low_momentum_stocks_monthly = pd.merge(low_momentum_stocks_monthly, low_momentum_stocks[['date', 'permno', 'mcap_lag1']], on=['date', 'permno'])

high_momentum_stocks_monthly['ew_return'] = high_momentum_stocks_monthly.groupby('date')['ret'].transform('mean')
high_momentum_stocks_monthly['vw_return'] = high_momentum_stocks_monthly.groupby('date').apply(
    lambda group: np.average(group['ret'], weights=group['mcap_lag1'])
).reset_index(level=0, drop=True)

low_momentum_stocks_monthly['ew_return'] = low_momentum_stocks_monthly.groupby('date')['ret'].transform('mean')
low_momentum_stocks_monthly['vw_return'] = low_momentum_stocks_monthly.groupby('date').apply(
    lambda group: np.average(group['ret'], weights=group['mcap_lag1'])
).reset_index(level=0, drop=True)


strategy_data_1 = pd.merge(high_momentum_stocks_monthly[['date', 'permno', 'ew_return', 'vw_return']],
                           low_momentum_stocks_monthly[['date', 'permno', 'ew_return', 'vw_return']],
                           on=['date'], suffixes=('_long', '_short'))

strategy_data_1['strategy_ew_return'] = strategy_data_1['ew_return_long'] - strategy_data_1['ew_return_short']
strategy_data_1['strategy_vw_return'] = strategy_data_1['vw_return_long'] - strategy_data_1['vw_return_short']






momentum_data['mom_bucket'] = pd.qcut(momentum_data['mom_intraday'] + momentum_data['mom_overnight'], 10, labels=False) + 1
momentum_data = momentum_data.dropna()
high_momentum_custom = momentum_data[momentum_data['mom_bucket'] == 10]
low_momentum_custom = momentum_data[momentum_data['mom_bucket'] == 1]

high_momentum_custom_monthly = high_momentum_custom.groupby(['date', 'permno'])['ret'].mean().reset_index()
low_momentum_custom_monthly = low_momentum_custom.groupby(['date', 'permno'])['ret'].mean().reset_index()

high_momentum_custom_monthly = pd.merge(high_momentum_custom_monthly, high_momentum_custom[['date', 'permno', 'mcap_lag1']], on=['date', 'permno'])
low_momentum_custom_monthly = pd.merge(low_momentum_custom_monthly, low_momentum_custom[['date', 'permno', 'mcap_lag1']], on=['date', 'permno'])

high_momentum_custom_monthly['ew_return'] = high_momentum_custom_monthly.groupby('date')['ret'].transform('mean')
high_momentum_custom_monthly['vw_return'] = high_momentum_custom_monthly.groupby('date').apply(
    lambda group: np.average(group['ret'], weights=group['mcap_lag1'])
).reset_index(level=0, drop=True)

low_momentum_custom_monthly['ew_return'] = low_momentum_custom_monthly.groupby('date')['ret'].transform('mean')
low_momentum_custom_monthly['vw_return'] = low_momentum_custom_monthly.groupby('date').apply(
    lambda group: np.average(group['ret'], weights=group['mcap_lag1'])
).reset_index(level=0, drop=True)

custom_strategy_data = pd.merge(high_momentum_custom_monthly[['date', 'ew_return', 'vw_return']],
                                low_momentum_custom_monthly[['date', 'ew_return', 'vw_return']],
                                on=['date'], suffixes=('_long', '_short'))

custom_strategy_data['strategy_ew_return'] = custom_strategy_data['ew_return_long'] - custom_strategy_data['ew_return_short']
custom_strategy_data['strategy_vw_return'] = custom_strategy_data['vw_return_long'] - custom_strategy_data['vw_return_short']





monthly_strategy_ew_return_1 = strategy_data_1.groupby('date')['strategy_ew_return'].mean()
monthly_strategy_vw_return_1 = strategy_data_1.groupby('date')['strategy_vw_return'].mean()

monthly_strategy_ew_return_custom = custom_strategy_data.groupby('date')['strategy_ew_return'].mean()
monthly_strategy_vw_return_custom = custom_strategy_data.groupby('date')['strategy_vw_return'].mean()





def compute_sharpe_ratio(strategy_return):
    excess_return = strategy_return.mean()
    volatility = strategy_return.std()
    return excess_return / volatility

sharpe_ew_1 = compute_sharpe_ratio(monthly_strategy_ew_return_1)
sharpe_vw_1 = compute_sharpe_ratio(monthly_strategy_vw_return_1)

sharpe_ew_custom = compute_sharpe_ratio(monthly_strategy_ew_return_custom)
sharpe_vw_custom = compute_sharpe_ratio(monthly_strategy_vw_return_custom)

print(f"Sharpe Ratio for Strategy 1 (EW): {sharpe_ew_1:.4f}")
print(f"Sharpe Ratio for Strategy 1 (VW): {sharpe_vw_1:.4f}")
print(f"Sharpe Ratio for Custom Strategy (EW): {sharpe_ew_custom:.4f}")
print(f"Sharpe Ratio for Custom Strategy (VW): {sharpe_vw_custom:.4f}")

plt.figure(figsize=(10, 6))
plt.plot(monthly_strategy_ew_return_1.index.astype(str), monthly_strategy_ew_return_1, label='Strategy 1 EW Return', color='blue')
plt.plot(monthly_strategy_vw_return_1.index.astype(str), monthly_strategy_vw_return_1, label='Strategy 1 VW Return', color='red')
plt.plot(monthly_strategy_ew_return_custom.index.astype(str), monthly_strategy_ew_return_custom, label='Custom Strategy EW Return', color='green')
plt.plot(monthly_strategy_vw_return_custom.index.astype(str), monthly_strategy_vw_return_custom, label='Custom Strategy VW Return', color='red')

### long short stategy total

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

def long_short_strategy_time_series(momentum_data):
    momentum_data['date'] = pd.to_datetime(momentum_data['date'])
    long_short_returns = []

    for month, group in momentum_data.groupby(momentum_data['date'].dt.to_period('M')):
        long_stocks = group[(group['intraday_bin'] == 5) & (group['overnight_bin'] == 5)]
        short_stocks = group[(group['intraday_bin'] == 1) & (group['overnight_bin'] == 1)]

        long_return = long_stocks['ret'].mean() if len(long_stocks) > 0 else 0
        short_return = short_stocks['ret'].mean() if len(short_stocks) > 0 else 0

        long_short_return = long_return - short_return
        long_short_returns.append((month, long_short_return))

    long_short_df = pd.DataFrame(long_short_returns, columns=['Month', 'Long_Short_Return'])
    long_short_df['Month'] = long_short_df['Month'].dt.to_timestamp()  # 转换回Timestamp以便绘图

    long_short_df['Cumulative_Return'] = (1 + long_short_df['Long_Short_Return']).cumprod() - 1

    return long_short_df

long_short_result = long_short_strategy_time_series(momentum_data)

print(long_short_result)

plt.figure(figsize=(10, 6))
plt.plot(long_short_result['Month'], long_short_result['Cumulative_Return'], label='Long/Short Strategy', color='b')
plt.xlabel('Date')
plt.ylabel('Cumulative Return')
plt.title('Long/Short Strategy Cumulative Return Over Time')
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

import numpy as np
import pandas as pd

def compute_sharpe_ratio(returns):
    return np.mean(returns) / np.std(returns) if np.std(returns) != 0 else 0

def calculate_sharpe(momentum_data):
    ew_grid, vw_grid = independent_sort(momentum_data)

    ew_monthly_returns = ew_grid.mean(axis=0)
    vw_monthly_returns = vw_grid.mean(axis=0)

    ew_sharpe = compute_sharpe_ratio(ew_monthly_returns)
    vw_sharpe = compute_sharpe_ratio(vw_monthly_returns)

    return ew_sharpe, vw_sharpe


ew_sharpe, vw_sharpe = calculate_sharpe(momentum_data)

print(f"Equal-weighted strategy Sharpe Ratio: {ew_sharpe}")
print(f"Value-weighted strategy Sharpe Ratio: {vw_sharpe}")

def long_short_custom_factor(momentum_data):
    long_stocks = momentum_data[(momentum_data['intraday_bin'] == 5) & (momentum_data['overnight_bin'] == 5)]
    short_stocks = momentum_data[(momentum_data['intraday_bin'] == 1) & (momentum_data['overnight_bin'] == 1)]

    long_return = long_stocks['ret'].mean() if len(long_stocks) > 0 else 0
    short_return = short_stocks['ret'].mean() if len(short_stocks) > 0 else 0

    long_short_return = long_return - short_return
    return long_short_return

def custom_factor_sharpe(momentum_data):
    custom_returns = []
    for month, group in momentum_data.groupby(momentum_data['date'].dt.to_period('M')):
        long_short_return = long_short_custom_factor(group)
        custom_returns.append(long_short_return)

    return compute_sharpe_ratio(custom_returns)

custom_sharpe = custom_factor_sharpe(momentum_data)

print(f"Custom long-short factor Sharpe Ratio: {custom_sharpe}")









### Q8:

import statsmodels.api as sm
import pandas as pd

# Step 1: Fama-MacBeth：overnight return -> future return
fama_macbeth_results = []

for date in momentum_data['date'].unique():
    subset = momentum_data[momentum_data['date'] == date]
    X = subset['overnight_ret_month']
    X = sm.add_constant(X)
    y = subset['ret']  # future returns

    model = sm.OLS(y, X).fit()
    fama_macbeth_results.append(model.params)

fama_macbeth_coefficients = pd.DataFrame(fama_macbeth_results)
fama_macbeth_avg = fama_macbeth_coefficients.mean()
fama_macbeth_t_stats = fama_macbeth_coefficients / fama_macbeth_coefficients.std()


# Step 2: Fama-MacBeth cross-section：intraday momentum -> return
cross_sectional_results = []

for date in momentum_data['date'].unique():
    subset = momentum_data[momentum_data['date'] == date]
    X = subset['mom_intraday']
    X = sm.add_constant(X)
    y = subset['ret']  # returns

    model = sm.OLS(y, X).fit()
    cross_sectional_results.append(model.params)

cross_sectional_coefficients = pd.DataFrame(cross_sectional_results)
cross_sectional_avg = cross_sectional_coefficients.mean()
cross_sectional_t_stats = cross_sectional_coefficients / cross_sectional_coefficients.std()

# Step 3: Interaction：overnight momentum * intraday momentum -> return
interaction_results = []

for date in momentum_data['date'].unique():
    subset = momentum_data[momentum_data['date'] == date]
    X = subset[['mom_intraday', 'mom_overnight']]
    X['interaction'] = X['mom_intraday'] * X['mom_overnight']  # 创建交互项
    X = sm.add_constant(X)
    y = subset['ret']  # returns

    model = sm.OLS(y, X).fit()
    interaction_results.append(model.params)  # 存储回归结果

interaction_coefficients = pd.DataFrame(interaction_results)
interaction_avg = interaction_coefficients.mean()
interaction_t_stats = interaction_coefficients / interaction_coefficients.std()

print("Fama-MacBeth Regression: \n", fama_macbeth_avg, fama_macbeth_t_stats)
print("Fama-MacBeth Cross-Sectional Regression: \n", cross_sectional_avg, cross_sectional_t_stats)
print("Interaction: \n", interaction_avg, interaction_t_stats)

import statsmodels.api as sm
import pandas as pd

momentum_data['date'] = pd.to_datetime(momentum_data['date'], errors='coerce')

long_stocks = momentum_data[(momentum_data['intraday_bin'] == 5) & (momentum_data['overnight_bin'] == 5)]
short_stocks = momentum_data[(momentum_data['intraday_bin'] == 1) & (momentum_data['overnight_bin'] == 1)]

momentum_portfolio_return = long_stocks['ret'].mean() - short_stocks['ret'].mean()

momentum_data['momentum_return'] = momentum_data.apply(
    lambda x: (x['ret'] if x['intraday_bin'] == 5 and x['overnight_bin'] == 5 else
               (-x['ret']) if x['intraday_bin'] == 1 and x['overnight_bin'] == 1 else 0),
    axis=1
)

merged_data = pd.merge(momentum_data, ff_factors, on='date', how='inner')



X_momentum = merged_data[['momentum_return']]
X_momentum = sm.add_constant(X_momentum)
y = merged_data['ret']

model_momentum = sm.OLS(y, X_momentum).fit()



X_ff5 = merged_data[['momentum_return', 'mkt_rf', 'smb', 'hml', 'rmw', 'cma']]
X_ff5 = sm.add_constant(X_ff5)
model_ff5 = sm.OLS(y, X_ff5).fit()

print("Mom Alpha:")
print(model_momentum.summary())

print("\nMom + FF5 Factors Alpha:")
print(model_ff5.summary())

import numpy as np
import pandas as pd
import statsmodels.api as sm

### Fama-Mecbeth
def fama_macbeth_regression(data, X_columns, y_column):
    coefficients = []
    t_statistics = []

    for date, group in data.groupby('date'):
        X = group[X_columns]
        X = sm.add_constant(X)
        y = group[y_column]

        model = sm.OLS(y, X).fit()

        coefficients.append(model.params)
        t_statistics.append(model.tvalues)

    coeff_df = pd.DataFrame(coefficients, columns=X_columns + ['const'])
    t_stat_df = pd.DataFrame(t_statistics, columns=X_columns + ['const'])

    mean_coeffs = coeff_df.mean()
    se_coeffs = coeff_df.std() / np.sqrt(len(coeff_df))

    mean_tstats = t_stat_df.mean()
    se_tstats = t_stat_df.std() / np.sqrt(len(t_stat_df))

    return mean_coeffs, se_coeffs, mean_tstats, se_tstats

merged_data['future_return'] = merged_data['ret'].shift(-1)

# 1. Overnight return and future return
X_overnight = ['overnight_ret_month']
y = 'future_return'
mean_coeffs_overnight, se_coeffs_overnight, mean_tstats_overnight, se_tstats_overnight = fama_macbeth_regression(
    merged_data, X_overnight, y
)

# 2. Intraday momentum and future return
X_intraday = ['mom_intraday']
mean_coeffs_intraday, se_coeffs_intraday, mean_tstats_intraday, se_tstats_intraday = fama_macbeth_regression(
    merged_data, X_intraday, y
)

# 3. Overnight return * Intraday momentum and future return
merged_data['interaction'] = merged_data['overnight_ret_month'] * merged_data['mom_intraday']
X_both = ['overnight_ret_month', 'mom_intraday', 'interaction']
mean_coeffs_both, se_coeffs_both, mean_tstats_both, se_tstats_both = fama_macbeth_regression(
    merged_data, X_both, y
)

results = pd.DataFrame({
    'Coefficient': pd.concat([mean_coeffs_overnight, mean_coeffs_intraday, mean_coeffs_both]),
    'Standard Error': pd.concat([se_coeffs_overnight, se_coeffs_intraday, se_coeffs_both]),
    't-statistic': pd.concat([mean_tstats_overnight, mean_tstats_intraday, mean_tstats_both]),
})


print(results)

